# Mini-Project for Fundamentals of Machine Learning Course
![background](./materials/ai_wp.jpg)
This repository contains the code and data for a mini-project on facial expression recognition using machine learning algorithms.

## üìë Project Policy
- Team: group should consist of 3-4 students.

    |No.| Student Name    | Student ID |
    | --------| -------- | ------- |
    |1| Nguy·ªÖn Minh Tr√≠ | 21110417 |
    |2| Nguy·ªÖn Ho√†ng Nam | 21110459 |
    |3| Ng√¥ VƒÉn Trung | 21110423 |
    |4| Tr√† Ho√†ng Tu·∫•n | 21110437 |

- The submission deadline is strict: **11:59 PM** on **June 22nd, 2024**. Commits pushed after this deadline will not be considered.

## üì¶ Project Structure

The repository is organized into the following directories:

- **/data**: This directory contains the facial expression dataset. You'll need to download the dataset and place it here before running the notebooks. (Download link provided below)
- **/notebooks**: This directory contains the Jupyter notebook ```EDA.ipynb```. This notebook guides you through exploratory data analysis (EDA) and classification tasks.

## ‚öôÔ∏è Usage

This project is designed to be completed in the following steps:

1. **Fork the Project**: Click on the ```Fork``` button on the top right corner of this repository, this will create a copy of the repository in your own GitHub account. Complete the table at the top by entering your team member names.

2. **Download the Dataset**: Download the facial expression dataset from the following [link](https://mega.nz/file/foM2wDaa#GPGyspdUB2WV-fATL-ZvYj3i4FqgbVKyct413gxg3rE) and place it in the **/data** directory:

3. **Complete the Tasks**: Open the ```notebooks/EDA.ipynb``` notebook in your Jupyter Notebook environment. The notebook is designed to guide you through various tasks, including:

    1. Prerequisite
    2. Principle Component Analysis
    3. Image Classification

        C√°c thu·∫≠t to√°n ph√¢n lo·∫°i nh√≥m t·ª•i em s·ª≠ d·ª•ng:
        - ‚úÖ Logistic Regression
        - ‚úÖ SVM
        - ‚úÖ MLP Classifier
        - ‚úÖ Random Forest
        - ‚úÖ CNN
    4. Evaluating Classification Performance

    Make sure to run all the code cells in the ```EDA.ipynb``` notebook and ensure they produce output before committing and pushing your changes.

5. **Commit and Push Your Changes**: Once you've completed the tasks outlined in the notebook, commit your changes to your local repository and push them to your forked repository on GitHub.


Feel free to modify and extend the notebook to explore further aspects of the data and experiment with different algorithms. Good luck.

---

# B√†i b√°o c√°o:
## I. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi ƒë∆∞·ª£c y√™u c·∫ßu:

**<u>C√¢u 1</u>:**
Can you visualize the data projected onto two principal components? (2 points)


‚úçÔ∏è V√¨ s·ªë principal components ƒë∆∞·ª£c y√™u c·∫ßu l√† 2, n√™n ta c√≥ th·ªÉ v·∫Ω ƒë∆∞·ª£c t·∫≠p d·ªØ li·ªáu khi gi·∫£m s·ªë chi·ªÅu t·ª´ s·ªë chi·ªÅu ban ƒë·∫ßu xu·ªëng ch·ªâ c√≤n 2 chi·ªÅu:

<p align="center">
  <img src="./materials/imgs/pca.png"/>
</p>



**<u>C√¢u 2</u>:** How to determine the optimal number of principal components using pca.explained_variance_? Explain your selection process. (2 points)

‚úçÔ∏è ƒê·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c s·ªë principal components t·ªëi ∆∞u th√¨ ph·∫£i x√°c ƒë·ªãnh m·ªôt ng∆∞·ª°ng t·ª∑ l·ªá ph∆∞∆°ng sai m√† t·∫°i ƒë√≥ d·ªØ li·ªáu ƒë∆∞·ª£c gi·∫£i th√≠ch nhi·ªÅu nh·∫•t. ·ªû ƒë√¢y nh√≥m t·ª•i em ch·ªçn ng∆∞·ª°ng c√≥ th·ªÉ gi·∫£i th√≠ch ƒë∆∞·ª£c l√† 90% ƒë·ªÉ ƒë·∫£m b·∫£o cho vi·ªác c√≥ th·ªÉ th·ª±c thi thu·∫≠t to√°n tr√™n m√°y t√≠nh m√† v·∫´n ƒë·∫£m b·∫£o d·ªØ li·ªáu qua PCA kh√¥ng qu√° m·∫•t qu√° nhi·ªÅu th√¥ng tin.

<p align="center">
  <img src="./materials/imgs/pca_ex.png"/>
</p>

ƒê·ªÉ gi·ªØ l·∫°i 90% t·ªïng ph∆∞∆°ng sai trong d·ªØ li·ªáu, ƒë∆∞·ªùng m√†u ƒë·ªè t·∫°i m·ª©c 90% ƒë·∫°i di·ªán cho ng∆∞·ª°ng ph∆∞∆°ng sai v·ªõi m·ª©c th√¥ng tin nh∆∞ v·∫≠y v√† ·ª©ng v·ªõi ng∆∞·ª°ng ƒë√≥ th√¨ s·ªë components l√† 103. Th√¥ng th∆∞·ªùng th√¨ th√¨ ta ch·ªçn ng∆∞·ªùng l√† 95% tuy nhi√™n v·ªõi ng∆∞·ª°ng n√†y th√¨ s·ªë components l√† 255 (m·ªôt s·ªë chi·ªÅu kh√° l·ªõn v√† g√¢y ·∫£nh h∆∞·ªüng t·ªõi t·ªëc ƒë·ªô ch·∫°y c·ªßa m√¥ h√¨nh).

**<u>C√¢u 3</u>:** Compare the performance of 4 different classification algorithms (3 machine learning and 1 MLP models) in both formats above. (4 points)

‚úçÔ∏è Ph·∫ßn n√†y nh√≥m t·ª•i em ƒë√£ ƒë√°nh gi√° kh√° chi ti·∫øt ·ªü trong file EDA.ipynb (**m·ª•c 4: Compare the performance of 4 different classification algorithms**) v√¨ ·ªü trong file ƒë√≥ c√≥ l∆∞u tr·ªØ k·∫øt qu·∫£ sau khi train c·ªßa t·ª´ng model, ti·ªán h∆°n cho vi·ªác so s√°nh.

Nh·ªØng n·ªôi dung ƒë√£ l√†m :

- ‚úÖ K·∫øt qu·∫£ sau khi train v·ªõi d·ªØ li·ªáu g·ªëc v√† qua PCA. (classification_report)

- ‚úÖ Tuning hyperparameters b·∫±ng GridSearch ƒë·ªëi v·ªõi d·ªØ li·ªáu g·ªëc v√† qua PCA.

- ‚úÖ Hi·ªán th·ªã Confusion Matrix cho t·ª´ng model ·ª©ng v·ªõi t·ª´ng d·ªØ li·ªáu g·ªëc v√† qua PCA.

- ‚úÖ So s√°nh v√† ƒë√°nh gi√° 4 m√¥ h√¨nh

**<u>C√¢u 4</u>:** Perform hyperparameter tuning using GridSearchCV for each classification method. (1 point)

‚úçÔ∏è ƒê·ªëi v·ªõi d·ªØ li·ªáu g·ªëc:

```py
LogisticRegression(C=0.1, solver='liblinear',max_iter=3500)

SVM(C=1, gamma=0.01, kernel='rbf', cv=2, verbose=1)

MLP(alpha=0.001, hidden_layer_sizes=(100,), max_iter=1000)

RandomForest(criterion='gini', n_estimators=500)
```

‚úçÔ∏è D·ªØ li·ªáu qua PCA:
```py
LogisticRegression(C=0.1, solver='lbfgs', max_iter=3500)

SVM(C=1,gamma='scale', kernel='rbf',cv=2, verbose=1)

MLP(alpha=0.01, hidden_layer_sizes=(50,), max_iter=1000)

RandomForest(criterion='gini', n_estimators=500)
```

**<u>C√¢u 5</u>:** Compare the performance of the different classification models using various metrics: accuracy, precision, recall, and F1-score. Based on the evaluation metrics, explain which model performs best and why. Identify the emotion category where the model makes the most accurate and most errors.

‚úçÔ∏è Ph·∫ßn n√†y nh√≥m t·ª•i em ƒë√£ ƒë√°nh gi√° kh√° chi ti·∫øt ·ªü trong file EDA.ipynb (**m·ª•c 5 Evaluating Classification Performance**)

## II. Nh·ªØng ƒëi·ªÅu th√∫ v·ªã trong qu√° tr√¨nh l√†m project

1. Trong qu√° tr√¨nh training t·ª•i em s·ª≠ d·ª•ng 4 thu·∫≠t to√°n bao g·ªìm (Logistic Regression, SVM, Random Forest, MLP) nh∆∞ng th·∫•y k·∫øt qu·∫£ train kh√° l√† th·∫•p v√† m·∫•t r·∫•t nhi·ªÅu th·ªùi gian ƒë·ªÉ c√≥ ƒë∆∞·ª£c k·∫øt qu·∫£, cho n√™n t·ª•i em l√†m th√™m m·ªôt thu·∫≠t to√°n CNN v√¨ thu·∫≠t to√°n n√†y x·ª≠ l√Ω t·ªët tr√™n nh·ªØng d·ªØ li·ªáu c√≥ c·∫•u tr√∫c phi tuy·∫øn nh∆∞ d·ªØ li·ªáu ·∫£nh (ph·∫ßn ƒë√°nh gi√° t·ª•i em ƒë·ªÉ ·ªü cu·ªëi file EDA). K·∫øt qu·∫£ train c≈©ng kh√¥ng h·∫≥n l√† qu√° cao nh∆∞ng v·∫´n cao h∆°n 4 thu·∫≠t to√°n tr√™n.

2. V√¨ t·ª•i em train tr√™n m√°y t√≠nh c√° nh√¢n, ƒë·ªÉ s·ª≠ d·ª•ng h·∫øt s·ªë CPU trong l√∫c d·ª± ƒëo√°n ```y_pred```, t·ª•i em c√≥ th√™m h√†m ```parallel_predict``` v·ªõi ```n_jobs = number of CPUs``` (n·∫øu kh√¥ng c√≥ h√†m n√†y th√¨ ch·∫Øc t·ª•i em train l√¢u h∆°n n·ªØa):
    ```py
    def parallel_predict(model, data, n_jobs=-1):
        n_splits = n_jobs if n_jobs > 0 else len(data)
        data_splits = np.array_split(data, n_splits)

        def predict_chunk(chunk):
            return model.predict(chunk)

        results = Parallel(n_jobs=n_jobs)(delayed(predict_chunk)(chunk) for chunk in data_splits)
        return np.concatenate(results)
    ```

**L∆∞u √Ω**: T·ª•i em c√≥ comment trong m·ªói cell code.
